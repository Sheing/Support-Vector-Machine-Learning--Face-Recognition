{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Sheing Jing Ng\n",
    "Project-4 CS147 Face Recognition\n",
    "\n",
    "For this assignment, you need to use a Support Vector Machine to do face recognition on the Labeled Faces in the Wild data set. You are expected to use at least the data for people with at least 70 images, though you can lower that number to include more people if you like. If so, make sure you say that you did so in your write-up, and give me a list of the names of the people in the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288\n",
      "['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\n",
      " 'Gerhard Schroeder' 'Hugo Chavez' 'Tony Blair']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70)\n",
    "print(len(lfw_people.images))\n",
    "print(lfw_people.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am using 70 min_faces per person. The result showed that there are a total of 7 names including Ariel Sharon, Colin Powell, Donald Rumsfeld, George W Bush, Gerhard Schroeder, Hugo Chavez and Tony Blair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################################################################################\n",
    "\n",
    "Part(1)\n",
    "\n",
    "(8/8 points) You are able to learn reasonably well (say an accuracy score of > 75%) on some variation in your experiments (though if you're using more faces, I'll accept a lower score due to the increased challenge).\n",
    "\n",
    "Part (2)\n",
    "(8/8 points) As a point of comparison get the accuracy of an SVM without doing any principal component analysis. For this part, you do not need to use a grid search as it will likely take a long time, but it would be nice to record the results of a few different kernels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83850931677\n",
      "0.422360248447\n",
      "0.844720496894\n",
      "0.422360248447\n"
     ]
    }
   ],
   "source": [
    "X = lfw_people.data\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "total=0\n",
    "count=0\n",
    "\n",
    "#Linear\n",
    "for i in range(10):\n",
    "    svc = svm.SVC(kernel='linear').fit(X_train, y_train)\n",
    "    y_pred1 = svc.predict(X_test)\n",
    "    total=total+ metrics.accuracy_score(y_test,y_pred1)\n",
    "    count=count+1\n",
    "average1=total/count\n",
    "print(average1)\n",
    "\n",
    "total=0\n",
    "count=0\n",
    "#RBF\n",
    "for j in range(10):\n",
    "    rbf_svc = svm.SVC(kernel='rbf').fit(X_train, y_train)\n",
    "    y_pred2=rbf_svc.predict(X_test)\n",
    "    total=total+ metrics.accuracy_score(y_test,y_pred2)\n",
    "    count=count+1\n",
    "average2=total/count\n",
    "print(average2)\n",
    "\n",
    "total=0\n",
    "count=0\n",
    "#Polynomial\n",
    "for n in range(10):\n",
    "    poly_svc = svm.SVC(kernel='poly').fit(X_train, y_train)\n",
    "    y_pred3=poly_svc.predict(X_test)\n",
    "    total=total+ metrics.accuracy_score(y_test,y_pred3)\n",
    "    count=count+1\n",
    "average3=total/count\n",
    "print(average3)\n",
    "\n",
    "total=0\n",
    "count=0\n",
    "#Sigmoid\n",
    "for k in range(10):\n",
    "    sigmoid_svc = svm.SVC(kernel='sigmoid').fit(X_train, y_train)\n",
    "    y_pred4=sigmoid_svc.predict(X_test)\n",
    "    total=total+ metrics.accuracy_score(y_test,y_pred4)\n",
    "    count=count+1\n",
    "average4=total/count\n",
    "print(average4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)According to the averaged result above, Linear and Polynomial kernel both has accuracy > 75%.\n",
    "Linear=83.85%,Polynomial=84.47%\n",
    "\n",
    "(2)I used 4 different types of kernel which is Linear,RBF,Polynomial and Sigmoid to experiment the result above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part (3)\n",
    "\n",
    "(8/8 points) Use principal component analysis to get the number of attributes down from 62*47=2914 attributes to somewhere around 100 attributes. Make sure you set the whiten parameter to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18943     0.14577259  0.07180682  0.05976154  0.05004617  0.02986242\n",
      "  0.02443048  0.02079068  0.02026307  0.01826063  0.015984    0.01467688\n",
      "  0.01185915  0.01120589  0.01018128  0.00989654  0.00918434  0.00884247\n",
      "  0.00805296  0.00708233  0.00680157  0.00666413  0.00599258  0.00581271\n",
      "  0.00552392  0.00526234  0.00511677  0.00476938  0.00469985  0.004258\n",
      "  0.00392809  0.00382161  0.00364395  0.0035281   0.00342332  0.0033083\n",
      "  0.00327476  0.00313952  0.00293515  0.00289004  0.00285125  0.00276304\n",
      "  0.00264347  0.00256512  0.00246808  0.00244351  0.00236744  0.00233807\n",
      "  0.00228841  0.00219331  0.002173    0.00215277  0.00203066  0.00202823\n",
      "  0.00199127  0.00192452  0.00188341  0.00183201  0.00178465  0.0017505\n",
      "  0.00170761  0.00167172  0.00161722  0.00158962  0.00155604  0.00150954\n",
      "  0.00149246  0.00148123  0.00143232  0.00141825  0.00140036  0.00137211\n",
      "  0.00134489  0.00132455  0.00128647  0.00126718  0.00124538  0.0012189\n",
      "  0.00120639  0.0011847   0.00114538  0.00114119  0.00112199  0.00111401\n",
      "  0.00109831  0.0010672   0.00103626  0.00102071  0.00100911  0.00100018\n",
      "  0.00096162  0.0009543   0.00093883  0.00092507  0.0009038   0.00089574\n",
      "  0.00088231  0.00086857  0.00081376  0.00079642]\n",
      "[ 0.01891939  0.01808006  0.01763689 ..., -0.03590741 -0.03506481\n",
      " -0.03262942]\n",
      "[-0.00245212  0.00784829  0.00491615 ...,  0.00863801 -0.00319186\n",
      "  0.00969435]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=100, svd_solver='randomized',whiten=True).fit(X_train)\n",
    "transformed_train_data = pca.transform(X_train)\n",
    "transformed_test_data =  pca.transform(X_test)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.components_[1])\n",
    "print(pca.components_[99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(8/8 points) Using your transformed data, use a grid search to find good sets of parameters to use for classification with an SVM. I expect that you search through at least 3 different kernels, each with > 4 reasonable values for associated kernel parameters. Since you will want to set the parameter grid differently for each of the different kernels, note that you can pass a list of the grid dictionaries to GridSearchCV() instead of just a single grid dictionary. Make sure to report in your write-up which sets of parameters resulted in the best classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C Parameter:\n",
      "0.01\n",
      "0.832298136646\n"
     ]
    }
   ],
   "source": [
    "from sklearn import grid_search\n",
    "##Linear\n",
    "parameters1 = {'C':[0.0001, 0.005,.001, 0.01,.1, 1]}\n",
    "svc_linear = grid_search.GridSearchCV(svm.SVC(kernel='linear'),parameters1).fit(transformed_train_data,y_train)\n",
    "print(\"Best C Parameter:\")\n",
    "print(svc_linear.best_estimator_.C)\n",
    "\n",
    "svc_linear_predict=svc_linear.predict(transformed_test_data)\n",
    "print(metrics.accuracy_score(svc_linear_predict,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C and Gamma Parameter:\n",
      "1\n",
      "0.01\n",
      "0.844720496894\n"
     ]
    }
   ],
   "source": [
    "##RBF\n",
    "parameters2= {'C':[0.0001, 0.005, 0.001, 0.1, 1],'gamma':[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]}\n",
    "svc_rbf = grid_search.GridSearchCV(svm.SVC(kernel='rbf'),parameters2).fit(transformed_train_data,y_train)\n",
    "\n",
    "print(\"Best C and Gamma Parameter:\")\n",
    "print(svc_rbf.best_estimator_.C)\n",
    "print(svc_rbf.best_estimator_.gamma)\n",
    "\n",
    "svc_rbf_predict=svc_rbf.predict(transformed_test_data)\n",
    "print(metrics.accuracy_score(svc_rbf_predict,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C,Gamma,Degree,Coef0 Parameters for Polynomial:\n",
      "1\n",
      "0.01\n",
      "2\n",
      "1\n",
      "0.844720496894\n"
     ]
    }
   ],
   "source": [
    "#Polynomial\n",
    "parameters3= {'C':[0.0001, 0.005, 0.001, 0.1, 1],'degree':[0,1,2,3,4,5],'gamma':[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],'coef0':[0.01, 0.1, 0, 1, 10,20]}\n",
    "svc_poly = grid_search.GridSearchCV(svm.SVC(kernel='poly'),parameters3).fit(transformed_train_data,y_train)\n",
    "print(\"Best C,Gamma,Degree,Coef0 Parameters for Polynomial:\")\n",
    "print(svc_poly.best_estimator_.C)\n",
    "print(svc_poly.best_estimator_.gamma)\n",
    "print(svc_poly.best_estimator_.degree)\n",
    "print(svc_poly.best_estimator_.coef0)\n",
    "\n",
    "svc_poly_predict=svc_poly.predict(transformed_test_data)\n",
    "print(metrics.accuracy_score(svc_poly_predict,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.01\n",
      "1\n",
      "0.0\n",
      "0.801242236025\n"
     ]
    }
   ],
   "source": [
    "#Sigmoid\n",
    "parameters4= {'C':[0.0001, 0.005, 0.001, 0.1, 1],'degree':[1,2,3,4,5],'gamma':[0.001, 0.005, 0.01, 0.1,0]}\n",
    "svc_sigmoid = grid_search.GridSearchCV(svm.SVC(kernel='sigmoid'),parameters4).fit(transformed_train_data,y_train)\n",
    "\n",
    "print(svc_sigmoid.best_estimator_.C)\n",
    "print(svc_sigmoid.best_estimator_.gamma)\n",
    "print(svc_sigmoid.best_estimator_.degree)\n",
    "print(svc_sigmoid.best_estimator_.coef0)\n",
    "\n",
    "svc_sigmoid_predict=svc_sigmoid.predict(transformed_test_data)\n",
    "print(metrics.accuracy_score(svc_sigmoid_predict,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the generated result, Polynomial and RBF both got the highest accuracy score at 84.47%, followed by Linear at 83.22% and lastly Sigmoid at 80.12%.\n",
    "\n",
    "We can conclude that after applying grid search to tweak the parameters, Polynomial with Parameter(C=1,Deg=2,Gamma=.01,Coef0=1) and RBF with C=1,Gamma=.01 are both the best classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################\n",
    "\n",
    "(8/8 points) Discussion of your results: In your write-up, make sure to give the results of whatever variations you tried (e.g., with and without doing PCA). Give some indication of the effect that doing PCA had on the experiment (did it seem to make it go faster? more accurate? worked with different kernels? etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before PCA           | After PCA\n",
    "---------------------|---------------------\n",
    "Polynomial: 84.47%   |Polynomial: 84.47%\n",
    "                     |\n",
    "Linear: 83.85%       |Linear:83.22%\n",
    "                     |\n",
    "RBF: 42.36%          |RBF: 84.47%\n",
    "                     |\n",
    "Sigmoid: 42.36%      |Sigmoid: 80.12%\n",
    "\n",
    "Analysis (Effect after PCA):\n",
    "Changes in Percentage: \n",
    "Polynomial: Unchanged\n",
    "Linear: -0.7%\n",
    "RBF:+99.41%\n",
    "Sigmoid: +89.14%\n",
    "\n",
    "RBF and Sigmoid has significant improvement on accuracy after applying PCA, especially RBF which increased from 42.36 to 84.47%. However, there's a slight decrease in accuracy for linear kernel, which decreased from 83.85% to 83.22%\n",
    "\n",
    "Time Performance:\n",
    "Polynomial performs faster and still maintain same high accuracy before PCA. Although PCA generated a same high accuracy score, but it is very time consuming. To run the results above, it took around 3-4 minutes compared to 10 secs to generate without using PCA.\n",
    "\n",
    "Linear performs better and have a higher accuracy score before PCA.\n",
    "\n",
    "Sigmoid and RBF performance improved a lot after applying PCA however, it took a long time to run under PCA, especially Sigmoid, when we have more parameters value to tweak.\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
